{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ##işletim sistemi ile ilgili görevleri yapıyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '4.3_notebook_quizz_pandas.ipynb',\n",
       " 'Automobile_data.csv',\n",
       " 'data üzerinde pandas ve regresyon çalışmaları.ipynb',\n",
       " 'Decisin Tree Regression.ipynb',\n",
       " 'Decision Tree',\n",
       " 'Decision Tree Classificaiton.ipynb',\n",
       " 'Decision Tree Classificationn_murat.ipynb',\n",
       " 'Decision_Tree_Reg.xlsx',\n",
       " 'KNN Regression.ipynb',\n",
       " 'Krill KNeighbors.ipynb',\n",
       " 'Krill LogReg.ipynb',\n",
       " 'Krill Naive Bayes Classification.ipynb',\n",
       " 'Krill SVM.ipynb',\n",
       " 'Linear Regression.ipynb',\n",
       " 'Logistic_regression.ipynb',\n",
       " 'meyve_agırlıkları_arastırması.csv',\n",
       " 'multiple linear regression.ipynb',\n",
       " 'Naive Bayes.ipynb',\n",
       " 'nearest neigbour.ipynb',\n",
       " 'Numpy Temel Konular (1).ipynb',\n",
       " 'Numpy Temel Konular (2).ipynb',\n",
       " 'Numpy Temel Konular.ipynb',\n",
       " 'optimizer_data.csv',\n",
       " 'pandas_calışma.ipynb',\n",
       " 'Polinomial2-Copy1.ipynb',\n",
       " 'Polinomial2.ipynb',\n",
       " 'PreprocessingAndDecisionTree.pk',\n",
       " 'PY0101EN-4-3-LoadData.ipynb',\n",
       " 'Random Forest Classification.ipynb',\n",
       " 'Random Forest Regression.rar',\n",
       " 'RandomForest.ipynb',\n",
       " 'S2upport Vector Regression.ipynb',\n",
       " 'Social_Network_Ads.csv',\n",
       " 'spam classifier.ipynb',\n",
       " 'Support Vector Machine.ipynb',\n",
       " 'Sİmple Linear Regression.ipynb',\n",
       " 'training_data.csv',\n",
       " 'Untitled2.ipynb',\n",
       " 'Untitled3.ipynb',\n",
       " 'Untitled4.ipynb',\n",
       " 'Untitled5.ipynb',\n",
       " 'Untitled6.ipynb',\n",
       " 'Untitled7.ipynb',\n",
       " 'XGBoost.ipynb']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir() ##içeriği listeliyor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Downloads\\\\enron\\\\enron1\\\\ham'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_dir=\"..\\..\\Downloads\\enron\\enron{}\\ham\".format(1) # mutlak yol olarak de verebiliriz.\n",
    "ham_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\gold\\\\Downloads\\\\python çalışmalar'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() ##current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_list=[]\n",
    "for mail in os.listdir(ham_dir):\n",
    "    with open(\"{}\\{}\".format(ham_dir,mail),\"r\", encoding='ansi') as file:\n",
    "        scan_mail=file.read()\n",
    "        ham_list.append(scan_mail)\n",
    "#print(scan_mail)\n",
    "#print(scanned_list[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3672"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: re : entex transistion\n",
      "thanks so much for the memo . i would like to reiterate my support on two key\n",
      "issues :\n",
      "1 ) . thu - best of luck on this new assignment . howard has worked hard and\n",
      "done a great job ! please don ' t be shy on asking questions . entex is\n",
      "critical to the texas business , and it is critical to our team that we are\n",
      "timely and accurate .\n",
      "2 ) . rita : thanks for setting up the account team . communication is\n",
      "critical to our success , and i encourage you all to keep each other informed\n",
      "at all times . the p & l impact to our business can be significant .\n",
      "additionally , this is high profile , so we want to assure top quality .\n",
      "thanks to all of you for all of your efforts . let me know if there is\n",
      "anything i can do to help provide any additional support .\n",
      "rita wynne\n",
      "12 / 14 / 99 02 : 38 : 45 pm\n",
      "to : janet h wallis / hou / ect @ ect , ami chokshi / corp / enron @ enron , howard b\n",
      "camp / hou / ect @ ect , thu nguyen / hou / ect @ ect , kyle r lilly / hou / ect @ ect , stacey\n",
      "neuweiler / hou / ect @ ect , george grant / hou / ect @ ect , julie meyers / hou / ect @ ect\n",
      "cc : daren j farmer / hou / ect @ ect , kathryn cordes / hou / ect @ ect , rita\n",
      "wynne / hou / ect , lisa csikos / hou / ect @ ect , brenda f herod / hou / ect @ ect , pamela\n",
      "chambers / corp / enron @ enron\n",
      "subject : entex transistion\n",
      "the purpose of the email is to recap the kickoff meeting held on yesterday\n",
      "with members from commercial and volume managment concernig the entex account :\n",
      "effective january 2000 , thu nguyen ( x 37159 ) in the volume managment group ,\n",
      "will take over the responsibility of allocating the entex contracts . howard\n",
      "and thu began some training this month and will continue to transition the\n",
      "account over the next few months . entex will be thu ' s primary account\n",
      "especially during these first few months as she learns the allocations\n",
      "process and the contracts .\n",
      "howard will continue with his lead responsibilites within the group and be\n",
      "available for questions or as a backup , if necessary ( thanks howard for all\n",
      "your hard work on the account this year ! ) .\n",
      "in the initial phases of this transistion , i would like to organize an entex\n",
      "\" account \" team . the team ( members from front office to back office ) would\n",
      "meet at some point in the month to discuss any issues relating to the\n",
      "scheduling , allocations , settlements , contracts , deals , etc . this hopefully\n",
      "will give each of you a chance to not only identify and resolve issues before\n",
      "the finalization process , but to learn from each other relative to your\n",
      "respective areas and allow the newcomers to get up to speed on the account as\n",
      "well . i would encourage everyone to attend these meetings initially as i\n",
      "believe this is a critical part to the success of the entex account .\n",
      "i will have my assistant to coordinate the initial meeting for early 1 / 2000 .\n",
      "if anyone has any questions or concerns , please feel free to call me or stop\n",
      "by . thanks in advance for everyone ' s cooperation . . . . . . . . . . .\n",
      "julie - please add thu to the confirmations distributions list\n"
     ]
    }
   ],
   "source": [
    "sample_mail=ham_list[9]\n",
    "print(sample_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(s):\n",
    "    pattern=\"[^A-Za-z]\"\n",
    "    return re.sub(pattern, \" \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject  re   entex transistion thanks so much for the memo   i would like to reiterate my support on two key issues         thu   best of luck on this new assignment   howard has worked hard and done a great job   please don   t be shy on asking questions   entex is critical to the texas business   and it is critical to our team that we are timely and accurate         rita   thanks for setting up the account team   communication is critical to our success   and i encourage you all to keep each other informed at all times   the p   l impact to our business can be significant   additionally   this is high profile   so we want to assure top quality   thanks to all of you for all of your efforts   let me know if there is anything i can do to help provide any additional support   rita wynne                           pm to   janet h wallis   hou   ect   ect   ami chokshi   corp   enron   enron   howard b camp   hou   ect   ect   thu nguyen   hou   ect   ect   kyle r lilly   hou   ect   ect   stacey neuweiler   hou   ect   ect   george grant   hou   ect   ect   julie meyers   hou   ect   ect cc   daren j farmer   hou   ect   ect   kathryn cordes   hou   ect   ect   rita wynne   hou   ect   lisa csikos   hou   ect   ect   brenda f herod   hou   ect   ect   pamela chambers   corp   enron   enron subject   entex transistion the purpose of the email is to recap the kickoff meeting held on yesterday with members from commercial and volume managment concernig the entex account   effective january        thu nguyen   x         in the volume managment group   will take over the responsibility of allocating the entex contracts   howard and thu began some training this month and will continue to transition the account over the next few months   entex will be thu   s primary account especially during these first few months as she learns the allocations process and the contracts   howard will continue with his lead responsibilites within the group and be available for questions or as a backup   if necessary   thanks howard for all your hard work on the account this year       in the initial phases of this transistion   i would like to organize an entex   account   team   the team   members from front office to back office   would meet at some point in the month to discuss any issues relating to the scheduling   allocations   settlements   contracts   deals   etc   this hopefully will give each of you a chance to not only identify and resolve issues before the finalization process   but to learn from each other relative to your respective areas and allow the newcomers to get up to speed on the account as well   i would encourage everyone to attend these meetings initially as i believe this is a critical part to the success of the entex account   i will have my assistant to coordinate the initial meeting for early            if anyone has any questions or concerns   please feel free to call me or stop by   thanks in advance for everyone   s cooperation                       julie   please add thu to the confirmations distributions list'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete(sample_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Downloads\\\\enron\\\\enron1\\\\spam'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dir=\"..\\..\\Downloads\\enron\\enron{}\\spam\".format(1) # mutlak yol olarak de verebiliriz.\n",
    "spam_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: ^ . pe , nis s ^ ize mat ; ters ! yhvqbvdboevkcd\n",
      "briababhdpr frdjvdbesk cdpizacqjkufx hfkosxcymgftzd wdyiwpbqipv xxieqncfpa\n",
      "the only solution to penis enlargement\n",
      "fxbekdcaolk gsiaagcrhyp\n",
      "limited offer : add at least 3 inches or get your money back !\n",
      "rlaegydzfb ylbafsepgjv\n",
      "we are so sure our product works we are willing to prove it by offering a free trial bottle + a 100 % money back guarantee upon purchase if you are not satisfied with the results .\n",
      "- - - > click here to learn more - - -\n",
      "also check out our * brand new * product : penis enlargement patches\n",
      "comes with the 100 % money back warranty as well !\n",
      "eqiupgbbaxz gogqkkdpbdo\n",
      "igjohodzauuuu yreliodctrin\n",
      "cbywdvdthl nogsvvbnwug\n",
      "no more offers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spam_list=[]\n",
    "for mail in os.listdir(spam_dir):\n",
    "    with open(\"{}\\{}\".format(spam_dir,mail),\"r\", encoding='ansi') as file:\n",
    "        scan_mail=file.read()\n",
    "        spam_list.append(scan_mail)\n",
    "#print(scan_mail)\n",
    "print(spam_list[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: ^ . pe , nis s ^ ize mat ; ters ! yhvqbvdboevkcd\n",
      "briababhdpr frdjvdbesk cdpizacqjkufx hfkosxcymgftzd wdyiwpbqipv xxieqncfpa\n",
      "the only solution to penis enlargement\n",
      "fxbekdcaolk gsiaagcrhyp\n",
      "limited offer : add at least 3 inches or get your money back !\n",
      "rlaegydzfb ylbafsepgjv\n",
      "we are so sure our product works we are willing to prove it by offering a free trial bottle + a 100 % money back guarantee upon purchase if you are not satisfied with the results .\n",
      "- - - > click here to learn more - - -\n",
      "also check out our * brand new * product : penis enlargement patches\n",
      "comes with the 100 % money back warranty as well !\n",
      "eqiupgbbaxz gogqkkdpbdo\n",
      "igjohodzauuuu yreliodctrin\n",
      "cbywdvdthl nogsvvbnwug\n",
      "no more offers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_spam_mail=spam_list[9]\n",
    "print(sample_spam_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject      pe   nis s   ize mat   ters   yhvqbvdboevkcd briababhdpr frdjvdbesk cdpizacqjkufx hfkosxcymgftzd wdyiwpbqipv xxieqncfpa the only solution to penis enlargement fxbekdcaolk gsiaagcrhyp limited offer   add at least   inches or get your money back   rlaegydzfb ylbafsepgjv we are so sure our product works we are willing to prove it by offering a free trial bottle   a       money back guarantee upon purchase if you are not satisfied with the results           click here to learn more       also check out our   brand new   product   penis enlargement patches comes with the       money back warranty as well   eqiupgbbaxz gogqkkdpbdo igjohodzauuuu yreliodctrin cbywdvdthl nogsvvbnwug no more offers '"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete(spam_list[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gold\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # Download stopwords so irrelevant words won't affect our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop=set(stopwords.words(\"english\"))\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(s):\n",
    "    word_list=s.split()\n",
    "    clean_word_list=[ps.stem(word).lower() for word in word_list if word not in stop]\n",
    "    return \" \".join(clean_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste=ham_list+spam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject christma tree farm pictur'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_list=[remove_stopwords(delete(mail)) for mail in liste]\n",
    "processed_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5172"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=len(ham_list)*[0]+len(spam_list)*[1]\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject entex transist thank much memo would like reiter support two key issu thu best luck new assign howard work hard done great job pleas shi ask question entex critic texa busi critic team time accur rita thank set account team commun critic success encourag keep inform time p l impact busi signific addit high profil want assur top qualiti thank effort let know anyth help provid addit support rita wynn pm janet h walli hou ect ect ami chokshi corp enron enron howard b camp hou ect ect thu nguyen hou ect ect kyle r lilli hou ect ect stacey neuweil hou ect ect georg grant hou ect ect juli meyer hou ect ect cc daren j farmer hou ect ect kathryn cord hou ect ect rita wynn hou ect lisa csiko hou ect ect brenda f herod hou ect ect pamela chamber corp enron enron subject entex transist purpos email recap kickoff meet held yesterday member commerci volum manag concernig entex account effect januari thu nguyen x volum manag group take respons alloc entex contract howard thu began train month continu transit account next month entex thu primari account especi first month learn alloc process contract howard continu lead responsibilit within group avail question backup necessari thank howard hard work account year initi phase transist would like organ entex account team team member front offic back offic would meet point month discuss issu relat schedul alloc settlement contract deal etc hope give chanc identifi resolv issu final process learn rel respect area allow newcom get speed account well would encourag everyon attend meet initi believ critic part success entex account assist coordin initi meet earli anyon question concern pleas feel free call stop thank advanc everyon cooper juli pleas add thu confirm distribut list'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(delete(sample_mail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=CountVectorizer(max_features=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5172, 1500)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(processed_list).toarray()\n",
    "x.shape # herbir satır mail, herbir kolonda da kelimeler var o mailde kaç defa geçtiğini gösteriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abil',\n",
       " 'abl',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accord',\n",
       " 'account',\n",
       " 'acquir',\n",
       " 'acquisit',\n",
       " 'across',\n",
       " 'act',\n",
       " 'action',\n",
       " 'activ',\n",
       " 'acton',\n",
       " 'actual',\n",
       " 'ad',\n",
       " 'add',\n",
       " 'addit',\n",
       " 'address',\n",
       " 'adjust',\n",
       " 'administr',\n",
       " 'adob',\n",
       " 'advanc',\n",
       " 'advantag',\n",
       " 'advertis',\n",
       " 'advic',\n",
       " 'advis',\n",
       " 'advisor',\n",
       " 'aep',\n",
       " 'aepin',\n",
       " 'affect',\n",
       " 'affili',\n",
       " 'afford',\n",
       " 'afternoon',\n",
       " 'age',\n",
       " 'agenc',\n",
       " 'agent',\n",
       " 'ago',\n",
       " 'agre',\n",
       " 'agreement',\n",
       " 'aime',\n",
       " 'air',\n",
       " 'al',\n",
       " 'albrecht',\n",
       " 'alert',\n",
       " 'ali',\n",
       " 'align',\n",
       " 'allen',\n",
       " 'alloc',\n",
       " 'allow',\n",
       " 'along',\n",
       " 'alreadi',\n",
       " 'also',\n",
       " 'altern',\n",
       " 'alway',\n",
       " 'america',\n",
       " 'american',\n",
       " 'ami',\n",
       " 'amount',\n",
       " 'analysi',\n",
       " 'analyst',\n",
       " 'anita',\n",
       " 'announc',\n",
       " 'annual',\n",
       " 'anoth',\n",
       " 'answer',\n",
       " 'anticip',\n",
       " 'anyon',\n",
       " 'anyth',\n",
       " 'aol',\n",
       " 'apach',\n",
       " 'appear',\n",
       " 'appli',\n",
       " 'applic',\n",
       " 'appreci',\n",
       " 'appropri',\n",
       " 'approv',\n",
       " 'approxim',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'archer',\n",
       " 'area',\n",
       " 'around',\n",
       " 'arrang',\n",
       " 'asap',\n",
       " 'ask',\n",
       " 'asset',\n",
       " 'assign',\n",
       " 'assist',\n",
       " 'associ',\n",
       " 'assum',\n",
       " 'assumpt',\n",
       " 'assur',\n",
       " 'attach',\n",
       " 'attend',\n",
       " 'attent',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'author',\n",
       " 'automat',\n",
       " 'avail',\n",
       " 'averag',\n",
       " 'avila',\n",
       " 'avoid',\n",
       " 'awar',\n",
       " 'award',\n",
       " 'away',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'balanc',\n",
       " 'bank',\n",
       " 'bankruptci',\n",
       " 'base',\n",
       " 'basi',\n",
       " 'basin',\n",
       " 'baumbach',\n",
       " 'baxter',\n",
       " 'beati',\n",
       " 'beaumont',\n",
       " 'becom',\n",
       " 'begin',\n",
       " 'believ',\n",
       " 'bellami',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'better',\n",
       " 'beverli',\n",
       " 'bgcolor',\n",
       " 'bid',\n",
       " 'big',\n",
       " 'bill',\n",
       " 'billion',\n",
       " 'bit',\n",
       " 'biz',\n",
       " 'black',\n",
       " 'blue',\n",
       " 'boa',\n",
       " 'board',\n",
       " 'bob',\n",
       " 'bodi',\n",
       " 'book',\n",
       " 'border',\n",
       " 'bought',\n",
       " 'box',\n",
       " 'br',\n",
       " 'brand',\n",
       " 'brandywin',\n",
       " 'brazo',\n",
       " 'break',\n",
       " 'brenda',\n",
       " 'brian',\n",
       " 'bridg',\n",
       " 'bring',\n",
       " 'broadband',\n",
       " 'broker',\n",
       " 'brown',\n",
       " 'browser',\n",
       " 'bruce',\n",
       " 'bryan',\n",
       " 'btu',\n",
       " 'budget',\n",
       " 'build',\n",
       " 'busi',\n",
       " 'button',\n",
       " 'buy',\n",
       " 'buyback',\n",
       " 'buyer',\n",
       " 'ca',\n",
       " 'calcul',\n",
       " 'calendar',\n",
       " 'california',\n",
       " 'call',\n",
       " 'calpin',\n",
       " 'came',\n",
       " 'camp',\n",
       " 'canada',\n",
       " 'cannot',\n",
       " 'capac',\n",
       " 'capit',\n",
       " 'captur',\n",
       " 'car',\n",
       " 'card',\n",
       " 'care',\n",
       " 'carlo',\n",
       " 'carri',\n",
       " 'carthag',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cass',\n",
       " 'categori',\n",
       " 'caus',\n",
       " 'cc',\n",
       " 'cd',\n",
       " 'cdnow',\n",
       " 'ce',\n",
       " 'cec',\n",
       " 'cell',\n",
       " 'center',\n",
       " 'central',\n",
       " 'ceo',\n",
       " 'cernosek',\n",
       " 'certain',\n",
       " 'certif',\n",
       " 'cf',\n",
       " 'chad',\n",
       " 'chairman',\n",
       " 'chanc',\n",
       " 'chang',\n",
       " 'channel',\n",
       " 'charg',\n",
       " 'charl',\n",
       " 'charlen',\n",
       " 'charli',\n",
       " 'charlott',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'cheryl',\n",
       " 'children',\n",
       " 'china',\n",
       " 'chokshi',\n",
       " 'choos',\n",
       " 'chri',\n",
       " 'christi',\n",
       " 'ciali',\n",
       " 'citi',\n",
       " 'citibank',\n",
       " 'claim',\n",
       " 'class',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'cleburn',\n",
       " 'clem',\n",
       " 'click',\n",
       " 'close',\n",
       " 'clyne',\n",
       " 'co',\n",
       " 'coastal',\n",
       " 'code',\n",
       " 'collect',\n",
       " 'color',\n",
       " 'colspan',\n",
       " 'column',\n",
       " 'com',\n",
       " 'combin',\n",
       " 'come',\n",
       " 'comment',\n",
       " 'commerci',\n",
       " 'commit',\n",
       " 'committe',\n",
       " 'common',\n",
       " 'commun',\n",
       " 'compani',\n",
       " 'compaq',\n",
       " 'compar',\n",
       " 'compens',\n",
       " 'competit',\n",
       " 'complet',\n",
       " 'complianc',\n",
       " 'comput',\n",
       " 'computron',\n",
       " 'concern',\n",
       " 'condit',\n",
       " 'confidenti',\n",
       " 'confirm',\n",
       " 'conflict',\n",
       " 'congratul',\n",
       " 'connect',\n",
       " 'consid',\n",
       " 'consult',\n",
       " 'consum',\n",
       " 'contact',\n",
       " 'contain',\n",
       " 'content',\n",
       " 'continu',\n",
       " 'contract',\n",
       " 'contribut',\n",
       " 'control',\n",
       " 'convers',\n",
       " 'coordin',\n",
       " 'copano',\n",
       " 'copi',\n",
       " 'corel',\n",
       " 'cornhusk',\n",
       " 'corp',\n",
       " 'corpor',\n",
       " 'correct',\n",
       " 'correctli',\n",
       " 'cost',\n",
       " 'cotten',\n",
       " 'cotton',\n",
       " 'could',\n",
       " 'counterparti',\n",
       " 'counti',\n",
       " 'countri',\n",
       " 'coupl',\n",
       " 'coupon',\n",
       " 'cours',\n",
       " 'cover',\n",
       " 'cp',\n",
       " 'cpr',\n",
       " 'creat',\n",
       " 'creativ',\n",
       " 'credit',\n",
       " 'critic',\n",
       " 'cross',\n",
       " 'crosstex',\n",
       " 'cs',\n",
       " 'csiko',\n",
       " 'current',\n",
       " 'custom',\n",
       " 'cut',\n",
       " 'cynthia',\n",
       " 'daili',\n",
       " 'dalla',\n",
       " 'dan',\n",
       " 'daniel',\n",
       " 'daren',\n",
       " 'darren',\n",
       " 'data',\n",
       " 'databas',\n",
       " 'date',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'day',\n",
       " 'de',\n",
       " 'deal',\n",
       " 'dealer',\n",
       " 'dear',\n",
       " 'dec',\n",
       " 'decemb',\n",
       " 'decid',\n",
       " 'decis',\n",
       " 'delet',\n",
       " 'deliv',\n",
       " 'deliveri',\n",
       " 'dell',\n",
       " 'delta',\n",
       " 'demand',\n",
       " 'demokrito',\n",
       " 'depart',\n",
       " 'deposit',\n",
       " 'descript',\n",
       " 'design',\n",
       " 'desk',\n",
       " 'detail',\n",
       " 'determin',\n",
       " 'develop',\n",
       " 'devon',\n",
       " 'dfarmer',\n",
       " 'die',\n",
       " 'differ',\n",
       " 'digit',\n",
       " 'direct',\n",
       " 'directli',\n",
       " 'director',\n",
       " 'discount',\n",
       " 'discrep',\n",
       " 'discuss',\n",
       " 'distribut',\n",
       " 'div',\n",
       " 'doc',\n",
       " 'doctor',\n",
       " 'document',\n",
       " 'dollar',\n",
       " 'donald',\n",
       " 'done',\n",
       " 'donna',\n",
       " 'door',\n",
       " 'dow',\n",
       " 'download',\n",
       " 'dr',\n",
       " 'draft',\n",
       " 'draw',\n",
       " 'drill',\n",
       " 'drive',\n",
       " 'drop',\n",
       " 'drug',\n",
       " 'dth',\n",
       " 'due',\n",
       " 'duke',\n",
       " 'duti',\n",
       " 'dvd',\n",
       " 'dy',\n",
       " 'dynegi',\n",
       " 'earl',\n",
       " 'earli',\n",
       " 'earn',\n",
       " 'easi',\n",
       " 'east',\n",
       " 'eastran',\n",
       " 'easttexa',\n",
       " 'eb',\n",
       " 'ect',\n",
       " 'ed',\n",
       " 'edit',\n",
       " 'educ',\n",
       " 'edward',\n",
       " 'ee',\n",
       " 'effect',\n",
       " 'effort',\n",
       " 'ei',\n",
       " 'eileen',\n",
       " 'either',\n",
       " 'el',\n",
       " 'elect',\n",
       " 'electr',\n",
       " 'elizabeth',\n",
       " 'els',\n",
       " 'email',\n",
       " 'employe',\n",
       " 'ena',\n",
       " 'encina',\n",
       " 'end',\n",
       " 'enerfin',\n",
       " 'energi',\n",
       " 'engag',\n",
       " 'enhanc',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'enron',\n",
       " 'enrononlin',\n",
       " 'enronxg',\n",
       " 'enserch',\n",
       " 'ensur',\n",
       " 'enter',\n",
       " 'enterpris',\n",
       " 'entex',\n",
       " 'entir',\n",
       " 'entiti',\n",
       " 'entri',\n",
       " 'environ',\n",
       " 'enw',\n",
       " 'eol',\n",
       " 'epgt',\n",
       " 'epson',\n",
       " 'equip',\n",
       " 'equistar',\n",
       " 'erect',\n",
       " 'eric',\n",
       " 'error',\n",
       " 'est',\n",
       " 'estim',\n",
       " 'etc',\n",
       " 'europ',\n",
       " 'even',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'evergreen',\n",
       " 'everi',\n",
       " 'everyon',\n",
       " 'everyth',\n",
       " 'ew',\n",
       " 'ex',\n",
       " 'exactli',\n",
       " 'exampl',\n",
       " 'except',\n",
       " 'excess',\n",
       " 'exchang',\n",
       " 'excit',\n",
       " 'exclus',\n",
       " 'execut',\n",
       " 'exist',\n",
       " 'expand',\n",
       " 'expect',\n",
       " 'expedia',\n",
       " 'expens',\n",
       " 'experi',\n",
       " 'experienc',\n",
       " 'expir',\n",
       " 'explor',\n",
       " 'export',\n",
       " 'express',\n",
       " 'ext',\n",
       " 'extend',\n",
       " 'extens',\n",
       " 'extra',\n",
       " 'exxon',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'facil',\n",
       " 'fact',\n",
       " 'factor',\n",
       " 'famili',\n",
       " 'far',\n",
       " 'fare',\n",
       " 'farmer',\n",
       " 'fast',\n",
       " 'fax',\n",
       " 'featur',\n",
       " 'feb',\n",
       " 'februari',\n",
       " 'feder',\n",
       " 'fee',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'field',\n",
       " 'file',\n",
       " 'fill',\n",
       " 'final',\n",
       " 'financ',\n",
       " 'financi',\n",
       " 'find',\n",
       " 'fine',\n",
       " 'firm',\n",
       " 'first',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'flash',\n",
       " 'floor',\n",
       " 'flow',\n",
       " 'focu',\n",
       " 'follow',\n",
       " 'font',\n",
       " 'forc',\n",
       " 'foreign',\n",
       " 'form',\n",
       " 'format',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'fred',\n",
       " 'free',\n",
       " 'fri',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'fuel',\n",
       " 'full',\n",
       " 'fun',\n",
       " 'function',\n",
       " 'fund',\n",
       " 'futur',\n",
       " 'fw',\n",
       " 'fwd',\n",
       " 'fyi',\n",
       " 'ga',\n",
       " 'gain',\n",
       " 'game',\n",
       " 'gari',\n",
       " 'gather',\n",
       " 'gave',\n",
       " 'gc',\n",
       " 'gco',\n",
       " 'gd',\n",
       " 'gdp',\n",
       " 'gener',\n",
       " 'georg',\n",
       " 'get',\n",
       " 'gif',\n",
       " 'gift',\n",
       " 'girl',\n",
       " 'gisb',\n",
       " 'give',\n",
       " 'given',\n",
       " 'global',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'god',\n",
       " 'goe',\n",
       " 'gold',\n",
       " 'golf',\n",
       " 'goliad',\n",
       " 'gome',\n",
       " 'good',\n",
       " 'got',\n",
       " 'govern',\n",
       " 'gpgfin',\n",
       " 'gr',\n",
       " 'grant',\n",
       " 'grave',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'green',\n",
       " 'greg',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growth',\n",
       " 'gtc',\n",
       " 'guarante',\n",
       " 'guess',\n",
       " 'gulf',\n",
       " 'guy',\n",
       " 'hakemack',\n",
       " 'half',\n",
       " 'hall',\n",
       " 'hand',\n",
       " 'handl',\n",
       " 'hank',\n",
       " 'hanson',\n",
       " 'happen',\n",
       " 'happi',\n",
       " 'hard',\n",
       " 'harri',\n",
       " 'hawkin',\n",
       " 'head',\n",
       " 'health',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heidi',\n",
       " 'height',\n",
       " 'held',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'henderson',\n",
       " 'hernandez',\n",
       " 'herod',\n",
       " 'herrera',\n",
       " 'hesco',\n",
       " 'hess',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'hill',\n",
       " 'hillari',\n",
       " 'histor',\n",
       " 'histori',\n",
       " 'hit',\n",
       " 'hold',\n",
       " 'holiday',\n",
       " 'holm',\n",
       " 'home',\n",
       " 'hope',\n",
       " 'hot',\n",
       " 'hotel',\n",
       " 'hotlist',\n",
       " 'hotmail',\n",
       " 'hou',\n",
       " 'hour',\n",
       " 'hous',\n",
       " 'houston',\n",
       " 'howard',\n",
       " 'howev',\n",
       " 'hpl',\n",
       " 'hplc',\n",
       " 'hplnl',\n",
       " 'hplno',\n",
       " 'hplo',\n",
       " 'hr',\n",
       " 'href',\n",
       " 'hsc',\n",
       " 'htm',\n",
       " 'html',\n",
       " 'htmlimg',\n",
       " 'http',\n",
       " 'hub',\n",
       " 'huge',\n",
       " 'hundr',\n",
       " 'ibm',\n",
       " 'ic',\n",
       " 'id',\n",
       " 'idea',\n",
       " 'identifi',\n",
       " 'iferc',\n",
       " 'iii',\n",
       " 'iit',\n",
       " 'im',\n",
       " 'imag',\n",
       " 'imbal',\n",
       " 'img',\n",
       " 'immedi',\n",
       " 'impact',\n",
       " 'implement',\n",
       " 'import',\n",
       " 'improv',\n",
       " 'inc',\n",
       " 'includ',\n",
       " 'increas',\n",
       " 'index',\n",
       " 'indic',\n",
       " 'individu',\n",
       " 'industri',\n",
       " 'info',\n",
       " 'inform',\n",
       " 'inher',\n",
       " 'initi',\n",
       " 'input',\n",
       " 'insid',\n",
       " 'instal',\n",
       " 'instant',\n",
       " 'instead',\n",
       " 'instruct',\n",
       " 'integr',\n",
       " 'intel',\n",
       " 'intend',\n",
       " 'intent',\n",
       " 'interconnect',\n",
       " 'interest',\n",
       " 'intern',\n",
       " 'internet',\n",
       " 'interview',\n",
       " 'intrast',\n",
       " 'invest',\n",
       " 'investor',\n",
       " 'invit',\n",
       " 'invoic',\n",
       " 'involv',\n",
       " 'issu',\n",
       " 'item',\n",
       " 'iv',\n",
       " 'jacki',\n",
       " 'jackson',\n",
       " 'jame',\n",
       " 'jan',\n",
       " 'janet',\n",
       " 'januari',\n",
       " 'jebel',\n",
       " 'jeff',\n",
       " 'jeffrey',\n",
       " 'jennif',\n",
       " 'jill',\n",
       " 'jim',\n",
       " 'jo',\n",
       " 'job',\n",
       " 'john',\n",
       " 'johnson',\n",
       " 'join',\n",
       " 'jone',\n",
       " 'jpg',\n",
       " 'juli',\n",
       " 'june',\n",
       " 'karen',\n",
       " 'katherin',\n",
       " 'kati',\n",
       " 'kc',\n",
       " 'keep',\n",
       " 'kelli',\n",
       " 'ken',\n",
       " 'kenneth',\n",
       " 'kevin',\n",
       " 'key',\n",
       " 'kill',\n",
       " 'kim',\n",
       " 'kimberli',\n",
       " 'kind',\n",
       " 'king',\n",
       " 'knle',\n",
       " 'know',\n",
       " 'known',\n",
       " 'koch',\n",
       " 'kristen',\n",
       " 'la',\n",
       " 'lake',\n",
       " 'lamadrid',\n",
       " 'lamphier',\n",
       " 'land',\n",
       " 'lannou',\n",
       " 'larg',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latest',\n",
       " 'launch',\n",
       " 'lauri',\n",
       " 'law',\n",
       " 'lead',\n",
       " 'leader',\n",
       " 'learn',\n",
       " 'least',\n",
       " 'leav',\n",
       " 'lee',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'less',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'level',\n",
       " 'licens',\n",
       " 'life',\n",
       " 'light',\n",
       " 'like',\n",
       " 'limit',\n",
       " 'linda',\n",
       " 'line',\n",
       " 'link',\n",
       " 'liquid',\n",
       " 'lisa',\n",
       " 'list',\n",
       " 'littl',\n",
       " 'live',\n",
       " 'liz',\n",
       " 'llc',\n",
       " 'lloyd',\n",
       " 'load',\n",
       " 'local',\n",
       " 'locat',\n",
       " 'logist',\n",
       " 'logo',\n",
       " 'london',\n",
       " 'lone',\n",
       " 'lonestar',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'lose',\n",
       " 'loss',\n",
       " 'lot',\n",
       " 'louisiana',\n",
       " 'love',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'lp',\n",
       " 'ls',\n",
       " 'lsk',\n",
       " 'lsp',\n",
       " 'lst',\n",
       " 'ltd',\n",
       " 'luong',\n",
       " 'macromedia',\n",
       " 'made',\n",
       " 'mail',\n",
       " 'mailto',\n",
       " 'maintain',\n",
       " 'mainten',\n",
       " 'major',\n",
       " 'make',\n",
       " 'man',\n",
       " 'manag',\n",
       " 'mani',\n",
       " 'manual',\n",
       " 'mar',\n",
       " 'march',\n",
       " 'mari',\n",
       " 'mark',\n",
       " 'market',\n",
       " 'marta',\n",
       " 'martin',\n",
       " 'match',\n",
       " 'materi',\n",
       " 'matt',\n",
       " 'matter',\n",
       " 'may',\n",
       " 'mccoy',\n",
       " 'mcf',\n",
       " 'mckay',\n",
       " 'mcmill',\n",
       " 'mean',\n",
       " 'measur',\n",
       " 'med',\n",
       " 'media',\n",
       " 'medic',\n",
       " 'meet',\n",
       " 'megan',\n",
       " 'melissa',\n",
       " 'member',\n",
       " 'memo',\n",
       " 'men',\n",
       " 'mention',\n",
       " 'messag',\n",
       " 'met',\n",
       " 'meter',\n",
       " 'methanol',\n",
       " 'meyer',\n",
       " 'mg',\n",
       " 'michael',\n",
       " 'microsoft',\n",
       " 'mid',\n",
       " 'midcon',\n",
       " 'middl',\n",
       " 'might',\n",
       " 'mike',\n",
       " 'mill',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'mine',\n",
       " 'minut',\n",
       " 'miss',\n",
       " 'mitchel',\n",
       " 'mm',\n",
       " 'mmbtu',\n",
       " 'mobil',\n",
       " 'model',\n",
       " 'monday',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'month',\n",
       " 'monthli',\n",
       " 'moopid',\n",
       " 'mop',\n",
       " 'morgan',\n",
       " 'morn',\n",
       " 'morri',\n",
       " 'move',\n",
       " 'movi',\n",
       " 'mr',\n",
       " 'ms',\n",
       " 'mtr',\n",
       " 'much',\n",
       " 'multipl',\n",
       " 'music',\n",
       " 'must',\n",
       " 'mx',\n",
       " 'na',\n",
       " 'name',\n",
       " 'nation',\n",
       " 'natur',\n",
       " 'nbsp',\n",
       " 'nd',\n",
       " 'neal',\n",
       " 'near',\n",
       " 'necessari',\n",
       " 'need',\n",
       " 'negoti',\n",
       " 'neon',\n",
       " 'net',\n",
       " 'network',\n",
       " 'neuweil',\n",
       " 'never',\n",
       " 'new',\n",
       " 'news',\n",
       " 'newslett',\n",
       " 'next',\n",
       " 'nguyen',\n",
       " 'night',\n",
       " 'nom',\n",
       " 'nomin',\n",
       " 'non',\n",
       " 'none',\n",
       " 'normal',\n",
       " 'north',\n",
       " 'note',\n",
       " 'noth',\n",
       " 'notic',\n",
       " 'notif',\n",
       " 'notifi',\n",
       " 'nov',\n",
       " 'novemb',\n",
       " 'number',\n",
       " 'oasi',\n",
       " 'object',\n",
       " 'obtain',\n",
       " 'occur',\n",
       " 'oct',\n",
       " 'octob',\n",
       " 'offer',\n",
       " 'offic',\n",
       " 'oi',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'old',\n",
       " 'olsen',\n",
       " 'one',\n",
       " 'onlin',\n",
       " 'oo',\n",
       " 'ook',\n",
       " 'open',\n",
       " 'oper',\n",
       " 'opinion',\n",
       " 'opportun',\n",
       " 'option',\n",
       " 'order',\n",
       " 'organ',\n",
       " 'origin',\n",
       " 'other',\n",
       " 'outag',\n",
       " 'outstand',\n",
       " 'owe',\n",
       " 'own',\n",
       " 'owner',\n",
       " 'packag',\n",
       " 'page',\n",
       " 'pager',\n",
       " 'paid',\n",
       " 'pain',\n",
       " 'paliourg',\n",
       " 'panenergi',\n",
       " 'papayoti',\n",
       " 'paper',\n",
       " 'paragraph',\n",
       " 'parent',\n",
       " 'park',\n",
       " 'parker',\n",
       " 'part',\n",
       " 'parti',\n",
       " 'particip',\n",
       " 'partner',\n",
       " 'paso',\n",
       " 'pass',\n",
       " 'password',\n",
       " 'past',\n",
       " 'pat',\n",
       " 'path',\n",
       " 'patti',\n",
       " 'paul',\n",
       " 'pay',\n",
       " 'payment',\n",
       " 'pc',\n",
       " 'pec',\n",
       " 'pef',\n",
       " 'peni',\n",
       " 'peopl',\n",
       " 'per',\n",
       " 'percent',\n",
       " 'perfect',\n",
       " 'perform',\n",
       " 'period',\n",
       " 'person',\n",
       " 'petroleum',\n",
       " 'pg',\n",
       " 'pharmaci',\n",
       " 'phillip',\n",
       " 'phone',\n",
       " ...]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  77,  78,  82,\n",
       "        83,  84,  85,  86,  87,  89,  90,  91,  93,  94,  95,  97, 102,\n",
       "       105, 106, 109, 121, 128, 130, 133, 134, 135, 138, 146, 154, 156,\n",
       "       165, 167, 169, 177, 180, 233, 273, 313, 340, 342, 701], dtype=int64)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(x) #kelimelerin ne kadar geçtiğini gösteriyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=x,columns=vectorizer.get_feature_names())\n",
    "X[\"label\"]=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>acquir</th>\n",
       "      <th>acquisit</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>xp</th>\n",
       "      <th>yahoo</th>\n",
       "      <th>ye</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yet</th>\n",
       "      <th>young</th>\n",
       "      <th>yvett</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014161</td>\n",
       "      <td>0.052832</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.053377</td>\n",
       "      <td>0.019063</td>\n",
       "      <td>0.093410</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.013617</td>\n",
       "      <td>0.008442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028322</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.082789</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.031590</td>\n",
       "      <td>0.058279</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>0.025599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.035333</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.020667</td>\n",
       "      <td>0.167333</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.043333</td>\n",
       "      <td>0.007333</td>\n",
       "      <td>0.137333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.032667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.139333</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.041333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           abil       abl    accept    access    accord   account    acquir  \\\n",
       "label                                                                         \n",
       "0      0.014161  0.052832  0.013617  0.053377  0.019063  0.093410  0.002451   \n",
       "1      0.028000  0.035333  0.056000  0.052000  0.020667  0.167333  0.036000   \n",
       "\n",
       "       acquisit    across       act  ...     xp     yahoo        ye      year  \\\n",
       "label                                ...                                        \n",
       "0      0.004085  0.013617  0.008442  ...  0.000  0.028322  0.014978  0.082789   \n",
       "1      0.043333  0.007333  0.137333  ...  0.112  0.032667  0.016667  0.139333   \n",
       "\n",
       "       yesterday       yet     young     yvett      zero      zone  \n",
       "label                                                               \n",
       "0       0.018519  0.031590  0.058279  0.015251  0.055011  0.025599  \n",
       "1       0.005333  0.018667  0.030000  0.001333  0.013333  0.041333  \n",
       "\n",
       "[2 rows x 1500 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_group=X.groupby(by=\"label\").mean()\n",
    "X_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3672\n",
       "1    1500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.709977\n",
       "1    0.290023\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"label\"].value_counts()/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goliad     inf\n",
       "dth        inf\n",
       "daren      inf\n",
       "neon       inf\n",
       "neuweil    inf\n",
       "dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X_group.loc[0])/(X_group.loc[1])).sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "specul      0.0\n",
       "oo          0.0\n",
       "ook         0.0\n",
       "paliourg    0.0\n",
       "knle        0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X_group.loc[0])/(X_group.loc[1])).sort_values(ascending=False).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "birler=np.ones((2,1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_smooth=np.concatenate((x, birler),axis=0)\n",
    "x_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_smooth=labels+[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_group=pd.DataFrame(data=x_smooth,columns=vectorizer.get_feature_names())\n",
    "X_group[\"labels\"]=y_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_group=X_group.groupby(\"labels\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enron           2679.160359\n",
       "hpl              947.677375\n",
       "daren            777.267084\n",
       "mmbtu            575.798802\n",
       "ect              473.293811\n",
       "sitara           352.263000\n",
       "hou              330.649545\n",
       "pec              307.719303\n",
       "ena              299.546148\n",
       "meter            277.785121\n",
       "nom              242.334059\n",
       "melissa          189.208549\n",
       "teco             181.852709\n",
       "tenaska          176.540158\n",
       "pat              163.054451\n",
       "aime             146.299483\n",
       "hsc              135.265723\n",
       "cotten           131.587803\n",
       "counterparti     128.318541\n",
       "chokshi          126.275252\n",
       "fyi              122.188674\n",
       "hplc             119.736727\n",
       "wellhead         117.693439\n",
       "clyne            113.198203\n",
       "eastran          111.972230\n",
       "txu               96.443234\n",
       "hplno             91.947999\n",
       "rita              91.130683\n",
       "lannou            90.722026\n",
       "buyback           81.731555\n",
       "                   ...     \n",
       "hotlist            0.005522\n",
       "moopid             0.005522\n",
       "div                0.005449\n",
       "valign             0.005108\n",
       "img                0.005045\n",
       "photoshop          0.005045\n",
       "pharmaci           0.005045\n",
       "mx                 0.004984\n",
       "prescript          0.004836\n",
       "biz                0.004394\n",
       "td                 0.004046\n",
       "bgcolor            0.003855\n",
       "php                0.003819\n",
       "oo                 0.003784\n",
       "voip               0.003616\n",
       "paliourg           0.003434\n",
       "height             0.003368\n",
       "soft               0.002898\n",
       "med                0.002878\n",
       "drug               0.002799\n",
       "width              0.002662\n",
       "ciali              0.002637\n",
       "src                0.002492\n",
       "xp                 0.002418\n",
       "font               0.002376\n",
       "viagra             0.002209\n",
       "href               0.002140\n",
       "computron          0.001682\n",
       "pill               0.001142\n",
       "nbsp               0.000975\n",
       "Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X_group.loc[0])/(X_group.loc[1])).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelM=MultinomialNB()\n",
    "modelG=GaussianNB()\n",
    "modelM.fit(x_smooth,y_smooth)\n",
    "modelG.fit(x_smooth,y_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9371616395978345\n",
      "0.9609435421500386\n"
     ]
    }
   ],
   "source": [
    "print(modelM.score(x,labels))\n",
    "print(modelG.score(x,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3447,  225],\n",
       "       [ 100, 1400]], dtype=int64)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmM=confusion_matrix(labels,modelM.predict(x))\n",
    "cmM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3480,  192],\n",
       "       [  10, 1490]], dtype=int64)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmG=confusion_matrix(labels,modelG.predict(x))\n",
    "cmG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3452,  220 gerçek  ham mailler\n",
    "#  107, 1393  gerçek spam mailler\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997134670487106"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3480 / (3480 + 10) # recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9477124183006536"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3480 / (3480 + 192) # precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enron6 için aynı işlemleri uygula ve predict score ve confusion_maxtirx değerlendir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"C:\\\\Users\\\\gold\\\\Desktop\\\\python programcıklar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sys.modules[\"spam_utils\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spam_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Downloads\\\\enron\\\\enron6\\\\ham'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_dir6=\"..\\..\\Downloads\\enron\\enron{}\\ham\".format(6) # mutlak yol olarak de verebiliriz.\n",
    "ham_dir6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Subject: key dates and impact of upcoming sap implementation\\nover the next few weeks , project apollo and beyond will conduct its final sap\\nimplementation \\x01 ) this implementation will impact approximately 12 , 000 new\\nusers plus all existing system users . sap brings a new dynamic to enron ,\\nenhancing the timely flow and sharing of specific project , human resources ,\\nprocurement , and financial information across business units and across\\ncontinents .\\nthis final implementation will retire multiple , disparate systems and replace\\nthem with a common , integrated system encompassing many processes including\\npayroll , timekeeping , benefits , project management , and numerous financial\\nprocesses .\\nemployees will be empowered to update and / or view their personal information\\nvia the intranet - based ehronline - - a single front - end to sap ' s self service\\nfunctionality and enron ' s global information system ( gis ) . among other\\nthings , individuals will be able to update personal information ( including\\nw - 4 , addresses and personal banking information ) , manage their individual\\ntime using a new time entry tool , view their benefit elections , and view\\ntheir personal payroll information on - line .\\nall enron employees paid out of corporate payroll in houston , excluding\\nazurix employees\\nthe financial communities of enron energy services , enron investment\\npartners , enron north america , enron renewable energy corporation , gas\\npipeline group , global finance , global it , enron networks , and global\\nproducts .\\nthe project management communities of enron north america , gas pipeline\\ngroup , global finance , global it , enron networks , and global products .\\nthe human resources communities of corporate , global e & p , enron energy\\nservices , enron engineering and construction company , enron investment\\npartners , enron north america , enron renewable energy corporation ( houston\\nonly ) , the international regions , gas pipeline group , global finance , global\\nit , enron networks , and global products .\\nexisting sap users currently supported by the center of expertise ( coe ) \\x01 )\\nincluding the london coe .\\npeople will be impacted gradually over the next few weeks :\\njune 12 - current sap users may notice ( and may use ) new features in some of\\nthe sap modules - - this new functionality was developed to meet requirements\\nof business units implementing sap as part of this final implementation .\\njune 22 - timekeeping functionality will be available for all employees paid\\nout of corporate payroll in houston ( excluding azurix employees ) .\\n- new sap coding must be used on timesheets .\\n- system ids will be available for all new users .\\njune 30 - deadline ! all time for the period beginning june 16 th and ending\\njune 30 th must be entered into sap by 3 : 00 cst .\\n- new sap coding must be used for all expenses and invoices .\\njuly 5 - all remaining functionality ( project management , financials , and\\nhuman resources ) are available to new end - users .\\nfor more information . . .\\nvisit us at an information booth in the enron building lobby on wednesday ,\\njune 7 th and thursday , june 8 th ( 10 a . m . till 2 p . m . each day . )\\nvisit our intranet site at http : \\\\ \\\\ sap . enron . com for job aids and other useful\\ninformation .\\ncontact the site manager coordinating the implementation within your business\\nunit or global function - - specific site manager contact information can be\\nfound on the intranet at http : \\\\ \\\\ sap . enron . com .\\ncontact the center of expertise ( coe ) for sap implementation and production\\nsupport questions via telephone at ( 713 ) 345 - 4 sap or via e - mail at\\nsap . coe @ enron . com .\""
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_list6=read_ham(ham_dir6)\n",
    "ham_list6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\..\\\\Downloads\\\\enron\\\\enron6\\\\spam'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dir6=\"..\\..\\Downloads\\enron\\enron{}\\spam\".format(6) # mutlak yol olarak de verebiliriz.\n",
    "spam_dir6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: advs\\ngreetings ,\\ni am benedicta lindiwe hendricks ( mrs ) of rsa . i am writing\\nthis letter to you with the hope that you will be kind enough\\nto assist my family .\\nif this means of communication is not acceptable to you please\\naccept my apologies as it is the only available and resourceful\\nmeans for me right now .\\nmy children and i are in need of your assistance and we sincerely\\npray and hope that you will be able to attend to our request .\\nif there is the possibility that you will be able to help us do\\nkindly let me know by return mail so that i can tell you about\\nour humble request .\\nthank for your understanding .\\nbenedicta lindiwe hendricks ( mrs ) .\\nplease reply to this email address ; heno 0 @ katamail . com'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_list6=read_spam(spam_dir6)\n",
    "spam_list6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste6=ham_list6+spam_list6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subject key date impact upcom sap implement next week project apollo beyond conduct final sap implement implement impact approxim new user plu exist system user sap bring new dynam enron enhanc time flow share specif project human resourc procur financi inform across busi unit across contin final implement retir multipl dispar system replac common integr system encompass mani process includ payrol timekeep benefit project manag numer financi process employe empow updat view person inform via intranet base ehronlin singl front end sap self servic function enron global inform system gi among thing individu abl updat person inform includ w address person bank inform manag individu time use new time entri tool view benefit elect view person payrol inform line enron employe paid corpor payrol houston exclud azurix employe financi commun enron energi servic enron invest partner enron north america enron renew energi corpor ga pipelin group global financ global enron network global product project manag commun enron north america ga pipelin group global financ global enron network global product human resourc commun corpor global e p enron energi servic enron engin construct compani enron invest partner enron north america enron renew energi corpor houston intern region ga pipelin group global financ global enron network global product exist sap user current support center expertis coe includ london coe peopl impact gradual next week june current sap user may notic may use new featur sap modul new function develop meet requir busi unit implement sap part final implement june timekeep function avail employe paid corpor payrol houston exclud azurix employe new sap code must use timesheet system id avail new user june deadlin time period begin june th end june th must enter sap cst new sap code must use expens invoic juli remain function project manag financi human resourc avail new end user inform visit us inform booth enron build lobbi wednesday june th thursday june th till p day visit intranet site http sap enron com job aid use inform contact site manag coordin implement within busi unit global function specif site manag contact inform found intranet http sap enron com contact center expertis coe sap implement product support question via telephon sap via e mail sap coe enron com'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_list6=[remove_StopWords(mail) for mail in liste6]\n",
    "processed_list6[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=create_labels(ham_list6,spam_list6)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set üzerinde inceleme yparken sadece transform yapıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=vectorizer.transform(processed_list6).toarray()\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9158333333333334\n",
      "0.9341666666666667\n"
     ]
    }
   ],
   "source": [
    "print(modelM.score(x_test,y_test))\n",
    "print(modelG.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1208,  292],\n",
       "       [ 103, 4397]], dtype=int64)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmG=confusion_matrix(y_test,modelG.predict(x_test))\n",
    "cmG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do you account for different prior probabilities for spam and ham\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=(0.5, 0.5), var_smoothing=1e-09)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelM=MultinomialNB()\n",
    "modelG=GaussianNB(priors=(0.5,0.5))\n",
    "modelM.fit(x_smooth,y_smooth)\n",
    "modelG.fit(x_smooth,y_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9158333333333334\n",
      "0.934\n"
     ]
    }
   ],
   "source": [
    "print(modelM.score(x_test,y_test))\n",
    "print(modelG.score(x_test,y_test)) # laplace smoothing scoreları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1207,  293],\n",
       "       [ 103, 4397]], dtype=int64)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmG=confusion_matrix(y_test,modelG.predict(x_test))\n",
    "cmG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelM=MultinomialNB()\n",
    "modelG=GaussianNB()\n",
    "modelM.fit(x,labels)\n",
    "modelG.fit(x,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9392884764114462\n",
      "0.9537896365042536\n"
     ]
    }
   ],
   "source": [
    "print(modelM.score(x,labels))\n",
    "print(modelG.score(x,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "print(modelM.score(x_test,y_test))\n",
    "print(modelG.score(x_test,y_test)) # without laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most discriminative words based on the learned probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enron           2679.160359\n",
       "hpl              947.677375\n",
       "daren            777.267084\n",
       "mmbtu            575.798802\n",
       "ect              473.293811\n",
       "sitara           352.263000\n",
       "hou              330.649545\n",
       "pec              307.719303\n",
       "ena              299.546148\n",
       "meter            277.785121\n",
       "nom              242.334059\n",
       "melissa          189.208549\n",
       "teco             181.852709\n",
       "tenaska          176.540158\n",
       "pat              163.054451\n",
       "aime             146.299483\n",
       "hsc              135.265723\n",
       "cotten           131.587803\n",
       "counterparti     128.318541\n",
       "chokshi          126.275252\n",
       "fyi              122.188674\n",
       "hplc             119.736727\n",
       "wellhead         117.693439\n",
       "clyne            113.198203\n",
       "eastran          111.972230\n",
       "txu               96.443234\n",
       "hplno             91.947999\n",
       "rita              91.130683\n",
       "lannou            90.722026\n",
       "buyback           81.731555\n",
       "                   ...     \n",
       "hotlist            0.005522\n",
       "moopid             0.005522\n",
       "div                0.005449\n",
       "valign             0.005108\n",
       "img                0.005045\n",
       "photoshop          0.005045\n",
       "pharmaci           0.005045\n",
       "mx                 0.004984\n",
       "prescript          0.004836\n",
       "biz                0.004394\n",
       "td                 0.004046\n",
       "bgcolor            0.003855\n",
       "php                0.003819\n",
       "oo                 0.003784\n",
       "voip               0.003616\n",
       "paliourg           0.003434\n",
       "height             0.003368\n",
       "soft               0.002898\n",
       "med                0.002878\n",
       "drug               0.002799\n",
       "width              0.002662\n",
       "ciali              0.002637\n",
       "src                0.002492\n",
       "xp                 0.002418\n",
       "font               0.002376\n",
       "viagra             0.002209\n",
       "href               0.002140\n",
       "computron          0.001682\n",
       "pill               0.001142\n",
       "nbsp               0.000975\n",
       "Length: 1500, dtype: float64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((X_group.loc[0])/(X_group.loc[1])).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9297538200339559"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=precision_score(y_test,modelG.predict(x_test)) # spam dediklerimizn gerçekte ne kadarı spam\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9735555555555555"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=recall_score(y_test,modelG.predict(x_test)) # bütün spamlein ne kadarını doğru tahmin ettik\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9511506730351715"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test,modelG.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: re : entex transistion\\nthanks so much for the memo . i would like to reiterate my support on two key\\nissues :\\n1 ) . thu - best of luck on this new assignment . howard has worked hard and\\ndone a great job ! please don \\' t be shy on asking questions . entex is\\ncritical to the texas business , and it is critical to our team that we are\\ntimely and accurate .\\n2 ) . rita : thanks for setting up the account team . communication is\\ncritical to our success , and i encourage you all to keep each other informed\\nat all times . the p & l impact to our business can be significant .\\nadditionally , this is high profile , so we want to assure top quality .\\nthanks to all of you for all of your efforts . let me know if there is\\nanything i can do to help provide any additional support .\\nrita wynne\\n12 / 14 / 99 02 : 38 : 45 pm\\nto : janet h wallis / hou / ect @ ect , ami chokshi / corp / enron @ enron , howard b\\ncamp / hou / ect @ ect , thu nguyen / hou / ect @ ect , kyle r lilly / hou / ect @ ect , stacey\\nneuweiler / hou / ect @ ect , george grant / hou / ect @ ect , julie meyers / hou / ect @ ect\\ncc : daren j farmer / hou / ect @ ect , kathryn cordes / hou / ect @ ect , rita\\nwynne / hou / ect , lisa csikos / hou / ect @ ect , brenda f herod / hou / ect @ ect , pamela\\nchambers / corp / enron @ enron\\nsubject : entex transistion\\nthe purpose of the email is to recap the kickoff meeting held on yesterday\\nwith members from commercial and volume managment concernig the entex account :\\neffective january 2000 , thu nguyen ( x 37159 ) in the volume managment group ,\\nwill take over the responsibility of allocating the entex contracts . howard\\nand thu began some training this month and will continue to transition the\\naccount over the next few months . entex will be thu \\' s primary account\\nespecially during these first few months as she learns the allocations\\nprocess and the contracts .\\nhoward will continue with his lead responsibilites within the group and be\\navailable for questions or as a backup , if necessary ( thanks howard for all\\nyour hard work on the account this year ! ) .\\nin the initial phases of this transistion , i would like to organize an entex\\n\" account \" team . the team ( members from front office to back office ) would\\nmeet at some point in the month to discuss any issues relating to the\\nscheduling , allocations , settlements , contracts , deals , etc . this hopefully\\nwill give each of you a chance to not only identify and resolve issues before\\nthe finalization process , but to learn from each other relative to your\\nrespective areas and allow the newcomers to get up to speed on the account as\\nwell . i would encourage everyone to attend these meetings initially as i\\nbelieve this is a critical part to the success of the entex account .\\ni will have my assistant to coordinate the initial meeting for early 1 / 2000 .\\nif anyone has any questions or concerns , please feel free to call me or stop\\nby . thanks in advance for everyone \\' s cooperation . . . . . . . . . . .\\njulie - please add thu to the confirmations distributions list'"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production(mail):\n",
    "    mail=remove_StopWords(mail)\n",
    "    mail_vectorized=vectorizer.transform([mail]).toarray()\n",
    "    pred_dict={0:\"ham\",1:\"spam\"}\n",
    "    return pred_dict[modelG.predict(mail_vectorized)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production(\"hi i am yusuf. 3, 6, ?, !, vi, ect, drugs's, enron, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "production(ham_list6[163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(modelG,open(\"spam_classifier.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer,open(\"spam_classifier_vectorizer.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
